## SVM、logistic regression、linear regression对比

- 线性回归 vs LR vs SVM：
线性回归做分类因为考虑了所有样本点到分类决策面的距离，所以在两类数据分布不均匀的时候将导致误差非常大；
LR和SVM克服了这个缺点，其中LR将所有数据采用sigmod函数进行了非线性映射，使得远离分类决策面的数据作用减弱；
SVM直接去掉了远离分类决策面的数据，只考虑支持向量的影响。

- 有异常点的时候，LR vs SVM：
对于这两种算法来说，在线性分类情况下，如果异常点较多无法剔除的话，LR中每个样本都是有贡献的，
最大似然后会自动压制异常的贡献；SVM+软间隔对异常比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。

- LR vs SVM 详细对比：
链接：https://zhuanlan.zhihu.com/p/30419036

  - 联系： \
1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） \
2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。\
3、LR和SVM都可以用来做非线性分类，只要加核函数就好。\
4、LR和SVM都是线性模型，当然这里我们不要考虑核函数\
5、都属于判别模型\
  - 区别：\
1、LR是参数模型，SVM是非参数模型。 \
2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。 \
3、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。\
4、SVM不直接依赖数据分布，而LR则依赖，因为SVM只与支持向量那几个点有关系，而LR和所有点都有关系。\
5、SVM依赖penalty系数，实验中需要做CV6、SVM本身是结构风险最小化模型，而LR是经验风险最小化模型 \

  - 另外怎么选模型：\
在Andrew NG的课里讲到过：\
1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM \
2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel \
3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况

